{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3de6c8ba-9f26-4dbd-91dd-f95f54575ae1",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1034078814.py, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[3], line 7\u001b[1;36m\u001b[0m\n\u001b[1;33m    tensorflow as tf\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "59bf4435-7f20-4fbc-80dd-10bd479dd2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def format_results(label, best_numbers):\n",
    "    #Formats weekly submission form \n",
    "    print(label)\n",
    "    print(np.array2string(np.array(best_numbers), precision=6, separator='-', floatmode='fixed', formatter={'float': '{:0.6f}'.format}))\n",
    "    \n",
    "def load_and_process_data(function_num=1, num_weeks=1):\n",
    "    # Load initial data\n",
    "    def load_data(i):\n",
    "        X = np.load(f'initial_data/function_{i}/initial_inputs.npy')\n",
    "        y = np.load(f'initial_data/function_{i}/initial_outputs.npy')\n",
    "        return X, y\n",
    "\n",
    "    def load_data_2(i):\n",
    "        X = np.load(f'initial_data2/function_{i}/initial_inputs.npy')\n",
    "        y = np.load(f'initial_data2/function_{i}/initial_outputs.npy')\n",
    "        return X, y\n",
    "        \n",
    "    X, y = load_data(function_num)\n",
    "    dimension = X.shape[1]\n",
    "    print(f\"Shape of initial X: {X.shape}\")\n",
    "    print(f\"Shape of initial y: {y.shape}\")\n",
    "\n",
    "    # Load and combine weekly data\n",
    "    for week in range(1, num_weeks + 1):\n",
    "        results_dir = f'results/week{week}'\n",
    "        X_week = np.load(f'{results_dir}/f{function_num}.npy')\n",
    "        y_week = np.load(f'{results_dir}/f_y{function_num}.npy')\n",
    "        \n",
    "        # Combine data\n",
    "        X = np.vstack((X, X_week))\n",
    "        y = np.concatenate((y, y_week))\n",
    "        \n",
    "        print(f\"After week {week}:\")\n",
    "        print(f\"Shape of X: {X.shape}\")\n",
    "        print(f\"Shape of y: {y.shape}\")\n",
    "\n",
    "    print(f\"\\nadded new data Batch week20\")\n",
    "    X2, y2 = load_data(function_num)\n",
    "    #dimension = X2.shape[1]\n",
    "    print(f\"Shape of initial X2: {X2.shape}\")\n",
    "    print(f\"Shape of initial y2: {y2.shape}\")\n",
    "    X = np.vstack((X, X2))\n",
    "    y = np.concatenate((y, y2))\n",
    "\n",
    "    # Create DataFrame\n",
    "    column_names = [f'X{i+1}' for i in range(X.shape[1])]\n",
    "    df = pd.DataFrame(X, columns=column_names)\n",
    "    df['y'] = y\n",
    "    \n",
    "    # Find maximum y value and corresponding row\n",
    "    max_y = df['y'].max()\n",
    "    max_row = df.loc[df['y'] == max_y]\n",
    "\n",
    "   \n",
    "    \n",
    "    print(f\"\\nFinal DataFrame shape: {df.shape}\")\n",
    "    print(df)\n",
    "    print(f\"\\nThe maximum y is: {max_y}\")\n",
    "    print(\"\\nThe row with the max y is:\")\n",
    "    print(max_row)\n",
    "    \n",
    "    return X, y, df, max_y, max_row.iloc[0]\n",
    "\n",
    "# Example usage\n",
    "#function_num = 1\n",
    "#num_weeks = 2  # Process data for 3 weeks\n",
    "#X, y, df, max_y, max_row = load_and_process_data(function_num, num_weeks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c49fc508-47fa-4a97-ae27-19f7577f3b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of initial X: (10, 2)\n",
      "Shape of initial y: (10,)\n",
      "After week 1:\n",
      "Shape of X: (11, 2)\n",
      "Shape of y: (11,)\n",
      "After week 2:\n",
      "Shape of X: (12, 2)\n",
      "Shape of y: (12,)\n",
      "After week 3:\n",
      "Shape of X: (13, 2)\n",
      "Shape of y: (13,)\n",
      "After week 4:\n",
      "Shape of X: (14, 2)\n",
      "Shape of y: (14,)\n",
      "After week 5:\n",
      "Shape of X: (15, 2)\n",
      "Shape of y: (15,)\n",
      "After week 6:\n",
      "Shape of X: (16, 2)\n",
      "Shape of y: (16,)\n",
      "After week 7:\n",
      "Shape of X: (17, 2)\n",
      "Shape of y: (17,)\n",
      "After week 8:\n",
      "Shape of X: (18, 2)\n",
      "Shape of y: (18,)\n",
      "\n",
      "added new data Batch week20\n",
      "Shape of initial X2: (10, 2)\n",
      "Shape of initial y2: (10,)\n",
      "\n",
      "Final DataFrame shape: (28, 3)\n",
      "          X1        X2              y\n",
      "0   0.319404  0.762959   1.322677e-79\n",
      "1   0.574329  0.879898   1.033078e-46\n",
      "2   0.731024  0.733000   7.710875e-16\n",
      "3   0.840353  0.264732  3.341771e-124\n",
      "4   0.650114  0.681526  -3.606063e-03\n",
      "5   0.410437  0.147554  -2.159249e-54\n",
      "6   0.312691  0.078723  -2.089093e-91\n",
      "7   0.683418  0.861057   2.535001e-40\n",
      "8   0.082507  0.403488   3.606771e-81\n",
      "9   0.883890  0.582254   6.229856e-48\n",
      "10  0.670114  0.701526   2.247051e-06\n",
      "11  0.668342  0.703518   4.145435e-07\n",
      "12  0.668342  0.778894  -6.367055e-19\n",
      "13  0.748744  0.648241   6.607457e-12\n",
      "14  0.748744  0.969849   9.181610e-93\n",
      "15  0.422111  0.984925  1.259310e-119\n",
      "16  0.979899  0.417085 -1.315233e-118\n",
      "17  0.620114  0.651526   4.385952e-01\n",
      "18  0.319404  0.762959   1.322677e-79\n",
      "19  0.574329  0.879898   1.033078e-46\n",
      "20  0.731024  0.733000   7.710875e-16\n",
      "21  0.840353  0.264732  3.341771e-124\n",
      "22  0.650114  0.681526  -3.606063e-03\n",
      "23  0.410437  0.147554  -2.159249e-54\n",
      "24  0.312691  0.078723  -2.089093e-91\n",
      "25  0.683418  0.861057   2.535001e-40\n",
      "26  0.082507  0.403488   3.606771e-81\n",
      "27  0.883890  0.582254   6.229856e-48\n",
      "\n",
      "The maximum y is: 0.43859515912233876\n",
      "\n",
      "The row with the max y is:\n",
      "          X1        X2         y\n",
      "17  0.620114  0.651526  0.438595\n"
     ]
    }
   ],
   "source": [
    "function_num = 1\n",
    "num_week = 8\n",
    "\n",
    "X, y, df, max_y, max_row = load_and_process_data(function_num, num_week)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b3df31-3975-463c-8125-f641aaf9669d",
   "metadata": {},
   "source": [
    "##  BayesianOptimizer class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "853be1f2-70b8-4362-9d16-927b6bf2386e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gpytorch\n",
    "import numpy as np\n",
    "from gpytorch.models import ExactGP\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from gpytorch.kernels import RBFKernel, ScaleKernel\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.stats import norm\n",
    "device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "class GPModel(ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(GPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = ScaleKernel(RBFKernel(ard_num_dims=train_x.shape[1]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "class BayesianOptimizer:\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.scaler = MinMaxScaler()\n",
    "        self.y_scaled = torch.tensor(self.scaler.fit_transform(y.reshape(-1, 1)).flatten(), dtype=torch.float32)\n",
    "        self.likelihood = GaussianLikelihood()\n",
    "        self.model = GPModel(self.X, self.y_scaled, self.likelihood)\n",
    "        \n",
    "    def train(self, training_iter=400, lr=0.1):\n",
    "        self.model.train()\n",
    "        self.likelihood.train()\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "        mll = ExactMarginalLogLikelihood(self.likelihood, self.model)\n",
    "        \n",
    "        for i in range(training_iter):\n",
    "            optimizer.zero_grad()\n",
    "            output = self.model(self.X)\n",
    "            loss = -mll(output, self.y_scaled)\n",
    "            loss.backward()\n",
    "            if i % 20 == 19:\n",
    "                print(f'Iter {i + 1}/{training_iter} - Loss: {loss.item():.4f}   Noise: {self.model.likelihood.noise.item():.4f}')\n",
    "            optimizer.step()\n",
    "        \n",
    "        self.model.eval()\n",
    "        self.likelihood.eval()\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_grid(X, num_points=100, hypercube_size=None, X_ini=None):\n",
    "        num_vars = X.shape[1]\n",
    "        \n",
    "        if hypercube_size is not None:\n",
    "            if X_ini is None:\n",
    "                center = X.mean(axis=0)\n",
    "            else:\n",
    "                center = X_ini\n",
    "            \n",
    "            ranges = []\n",
    "            for i in range(num_vars):\n",
    "                lower_bound = max(0, center[i] - hypercube_size/2)\n",
    "                upper_bound = min(1, center[i] + hypercube_size/2)\n",
    "                ranges.append(np.linspace(lower_bound, upper_bound, num_points))\n",
    "        else:\n",
    "            ranges = [np.linspace(X[:, i].min(), X[:, i].max(), num_points) for i in range(num_vars)]\n",
    "        \n",
    "        grid = np.array(np.meshgrid(*ranges)).T.reshape(-1, num_vars)\n",
    "        return grid\n",
    "    \n",
    "    @staticmethod\n",
    "    def expected_improvement(mean, sigma, best_f, xi=0.01):\n",
    "        with torch.no_grad():\n",
    "            z = (mean - best_f - xi) / sigma\n",
    "            normal = torch.distributions.Normal(torch.zeros_like(z), torch.ones_like(z))\n",
    "            ei = (mean - best_f - xi) * normal.cdf(z) + sigma * normal.log_prob(z).exp()\n",
    "        return ei\n",
    "    \n",
    "    def optimize(self, num_points=50, hypercube_size=0.5, X_ini=None):\n",
    "        if X_ini is None:\n",
    "            X_ini = self.X[self.y_scaled.argmax()]\n",
    "        \n",
    "        grid = self.create_grid(self.X.numpy(), num_points, hypercube_size, X_ini.numpy())\n",
    "        grid_tensor = torch.tensor(grid, dtype=torch.float32)\n",
    "        \n",
    "        with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "            predictions = self.likelihood(self.model(grid_tensor))\n",
    "            mean = predictions.mean\n",
    "            sigma = predictions.stddev\n",
    "        \n",
    "        best_f = self.y_scaled.max().item()\n",
    "        ei = self.expected_improvement(mean, sigma, best_f)\n",
    "        \n",
    "        max_ei_value, max_ei_index = ei.max(0)\n",
    "        max_ei_input = grid[max_ei_index]\n",
    "        max_ei_prediction = mean[max_ei_index].item()\n",
    "        max_ei_prediction_original = self.scaler.inverse_transform([[max_ei_prediction]])[0][0]\n",
    "        \n",
    "        actual_max_value = self.y_scaled.max().item()\n",
    "        actual_max_value_original = self.scaler.inverse_transform([[actual_max_value]])[0][0]\n",
    "        actual_max_index = self.y_scaled.argmax().item()\n",
    "        actual_max_input = self.X[actual_max_index].numpy()\n",
    "        \n",
    "        results = {\n",
    "            \"max_ei_input\": max_ei_input,\n",
    "            \"max_ei_prediction\": max_ei_prediction_original,\n",
    "            \"max_ei_value\": max_ei_value.item(),\n",
    "            \"actual_max_input\": actual_max_input,\n",
    "            \"actual_max_value\": actual_max_value_original\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Example usage:\n",
    "# optimizer = BayesianOptimizer(X, y)\n",
    "# optimizer.train()\n",
    "# results = optimizer.optimize()\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "29c44b06-b5d2-4a91-b31d-7e9c2abf4e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\claud\\AppData\\Local\\Temp\\ipykernel_115208\\2251220785.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.X = torch.tensor(X, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 20/500 - Loss: 0.1244   Noise: 0.1373\n",
      "Iter 40/500 - Loss: -0.2045   Noise: 0.0292\n",
      "Iter 60/500 - Loss: -0.2429   Noise: 0.0361\n",
      "Iter 80/500 - Loss: -0.2584   Noise: 0.0349\n",
      "Iter 100/500 - Loss: -0.2659   Noise: 0.0343\n",
      "Iter 120/500 - Loss: -0.2683   Noise: 0.0342\n",
      "Iter 140/500 - Loss: -0.2700   Noise: 0.0341\n",
      "Iter 160/500 - Loss: -0.2707   Noise: 0.0341\n",
      "Iter 180/500 - Loss: -0.2711   Noise: 0.0340\n",
      "Iter 200/500 - Loss: -0.2715   Noise: 0.0340\n",
      "Iter 220/500 - Loss: -0.2717   Noise: 0.0340\n",
      "Iter 240/500 - Loss: -0.2719   Noise: 0.0340\n",
      "Iter 260/500 - Loss: -0.2716   Noise: 0.0340\n",
      "Iter 280/500 - Loss: -0.2721   Noise: 0.0340\n",
      "Iter 300/500 - Loss: -0.2722   Noise: 0.0340\n",
      "Iter 320/500 - Loss: -0.2723   Noise: 0.0340\n",
      "Iter 340/500 - Loss: -0.2723   Noise: 0.0340\n",
      "Iter 360/500 - Loss: -0.2724   Noise: 0.0340\n",
      "Iter 380/500 - Loss: -0.2724   Noise: 0.0340\n",
      "Iter 400/500 - Loss: -0.2725   Noise: 0.0339\n",
      "Iter 420/500 - Loss: -0.2725   Noise: 0.0339\n",
      "Iter 440/500 - Loss: -0.2725   Noise: 0.0339\n",
      "Iter 460/500 - Loss: -0.2722   Noise: 0.0340\n",
      "Iter 480/500 - Loss: -0.2725   Noise: 0.0340\n",
      "Iter 500/500 - Loss: -0.2726   Noise: 0.0339\n",
      "Optimization Results:\n",
      "Best point found: [0.63746097 0.70152597]\n",
      "Predicted value at best point (scaled): 0.04322966933250427\n",
      "Predicted value at best point (original scale): 0.01551014995104045\n",
      "Expected Improvement: -8.881428925633372e-09\n",
      "\n",
      "Actual best point in training data: [0.620114 0.651526]\n",
      "Actual best value in training data (scaled): 1.0\n",
      "Actual best value in training data (original scale): 0.4385951591223387\n",
      "\n",
      "Function Number: 1\n",
      "Next query:\n",
      "[0.637461-0.701526]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#from bayesian_optimizer import BayesianOptimizer\n",
    "\n",
    "# Scale y values\n",
    "scaler = MinMaxScaler()\n",
    "y_scaled = scaler.fit_transform(y.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Convert numpy arrays to torch tensors\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y_scaled = torch.tensor(y_scaled, dtype=torch.float32)\n",
    "\n",
    "# Create and use the optimizer\n",
    "optimizer = BayesianOptimizer(X, y_scaled)\n",
    "optimizer.train(training_iter=500)\n",
    "results = optimizer.optimize(num_points=50, hypercube_size=0.1)\n",
    "\n",
    "print(\"Optimization Results:\")\n",
    "print(f\"Best point found: {results['max_ei_input']}\")\n",
    "print(f\"Predicted value at best point (scaled): {results['max_ei_prediction']}\")\n",
    "print(f\"Predicted value at best point (original scale): {scaler.inverse_transform([[results['max_ei_prediction']]])[0][0]}\")\n",
    "print(f\"Expected Improvement: {results['max_ei_value']}\")\n",
    "print(f\"\\nActual best point in training data: {results['actual_max_input']}\")\n",
    "print(f\"Actual best value in training data (scaled): {results['actual_max_value']}\")\n",
    "print(f\"Actual best value in training data (original scale): {scaler.inverse_transform([[results['actual_max_value']]])[0][0]}\")\n",
    "print('\\nFunction Number:',function_num)\n",
    "format_results('Next query:', results['max_ei_input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "adfd566a-2e29-4c50-b764-aa155112f6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of initial X: (10, 2)\n",
      "Shape of initial y: (10,)\n",
      "After week 1:\n",
      "Shape of X: (11, 2)\n",
      "Shape of y: (11,)\n",
      "After week 2:\n",
      "Shape of X: (12, 2)\n",
      "Shape of y: (12,)\n",
      "After week 3:\n",
      "Shape of X: (13, 2)\n",
      "Shape of y: (13,)\n",
      "After week 4:\n",
      "Shape of X: (14, 2)\n",
      "Shape of y: (14,)\n",
      "After week 5:\n",
      "Shape of X: (15, 2)\n",
      "Shape of y: (15,)\n",
      "After week 6:\n",
      "Shape of X: (16, 2)\n",
      "Shape of y: (16,)\n",
      "After week 7:\n",
      "Shape of X: (17, 2)\n",
      "Shape of y: (17,)\n",
      "After week 8:\n",
      "Shape of X: (18, 2)\n",
      "Shape of y: (18,)\n",
      "\n",
      "added new data Batch week20\n",
      "Shape of initial X2: (10, 2)\n",
      "Shape of initial y2: (10,)\n",
      "\n",
      "Final DataFrame shape: (28, 3)\n",
      "          X1        X2         y\n",
      "0   0.665800  0.123969  0.538996\n",
      "1   0.877791  0.778628  0.420586\n",
      "2   0.142699  0.349005 -0.065624\n",
      "3   0.845275  0.711120  0.293993\n",
      "4   0.454647  0.290455  0.214965\n",
      "5   0.577713  0.771973  0.023106\n",
      "6   0.438166  0.685018  0.244619\n",
      "7   0.341750  0.028698  0.038749\n",
      "8   0.338648  0.213867 -0.013858\n",
      "9   0.702637  0.926564  0.611205\n",
      "10  0.775510  0.959184  0.023768\n",
      "11  0.714286  0.938776  0.451086\n",
      "12  0.693878  0.897959  0.563238\n",
      "13  0.693878  0.918367  0.521920\n",
      "14  0.714286  0.918367  0.490105\n",
      "15  0.693878  0.938776  0.636626\n",
      "16  0.693878  0.877551  0.583641\n",
      "17  0.684694  0.988776  0.593574\n",
      "18  0.665800  0.123969  0.538996\n",
      "19  0.877791  0.778628  0.420586\n",
      "20  0.142699  0.349005 -0.065624\n",
      "21  0.845275  0.711120  0.293993\n",
      "22  0.454647  0.290455  0.214965\n",
      "23  0.577713  0.771973  0.023106\n",
      "24  0.438166  0.685018  0.244619\n",
      "25  0.341750  0.028698  0.038749\n",
      "26  0.338648  0.213867 -0.013858\n",
      "27  0.702637  0.926564  0.611205\n",
      "\n",
      "The maximum y is: 0.636626128478489\n",
      "\n",
      "The row with the max y is:\n",
      "          X1        X2         y\n",
      "15  0.693878  0.938776  0.636626\n"
     ]
    }
   ],
   "source": [
    "function_num = 2\n",
    "num_week = 8\n",
    "\n",
    "X, y, df, max_y, max_row = load_and_process_data(function_num, num_week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f30821e1-5dc3-4a77-9b58-bd6091dc66f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\claud\\AppData\\Local\\Temp\\ipykernel_115208\\2377261861.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X = torch.tensor(X, dtype=torch.float32)\n",
      "C:\\Users\\claud\\AppData\\Local\\Temp\\ipykernel_115208\\2251220785.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.X = torch.tensor(X, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 20/200 - Loss: 0.2846   Noise: 0.1404\n",
      "Iter 40/200 - Loss: -0.2425   Noise: 0.0339\n",
      "Iter 60/200 - Loss: -0.7797   Noise: 0.0054\n",
      "Iter 80/200 - Loss: -0.9258   Noise: 0.0015\n",
      "Iter 100/200 - Loss: -0.9372   Noise: 0.0018\n",
      "Iter 120/200 - Loss: -0.9385   Noise: 0.0019\n",
      "Iter 140/200 - Loss: -0.9394   Noise: 0.0018\n",
      "Iter 160/200 - Loss: -0.9399   Noise: 0.0019\n",
      "Iter 180/200 - Loss: -0.9402   Noise: 0.0018\n",
      "Iter 200/200 - Loss: -0.9405   Noise: 0.0018\n",
      "Optimization Results:\n",
      "Best point found: [0.68622493 1.        ]\n",
      "Predicted value at best point (scaled): 0.9421074390411377\n",
      "Predicted value at best point (original scale): 0.5959710918494635\n",
      "Expected Improvement: 0.0015967614017426968\n",
      "\n",
      "Actual best point in training data: [0.693878 0.938776]\n",
      "Actual best value in training data (scaled): 1.0\n",
      "Actual best value in training data (original scale): 0.636626128478489\n",
      "\n",
      "Function Number: 2\n",
      "Next query:\n",
      "[0.686225-1.000000]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Scale y values\n",
    "scaler = MinMaxScaler()\n",
    "y_scaled = scaler.fit_transform(y.reshape(-1, 1)).flatten()\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y_scaled = torch.tensor(y_scaled, dtype=torch.float32)\n",
    "\n",
    "# Create and use the optimizer\n",
    "optimizer = BayesianOptimizer(X, y_scaled)\n",
    "optimizer.train(training_iter=200)\n",
    "results = optimizer.optimize(num_points=50, hypercube_size=0.15)\n",
    "\n",
    "print(\"Optimization Results:\")\n",
    "print(f\"Best point found: {results['max_ei_input']}\")\n",
    "print(f\"Predicted value at best point (scaled): {results['max_ei_prediction']}\")\n",
    "print(f\"Predicted value at best point (original scale): {scaler.inverse_transform([[results['max_ei_prediction']]])[0][0]}\")\n",
    "print(f\"Expected Improvement: {results['max_ei_value']}\")\n",
    "print(f\"\\nActual best point in training data: {results['actual_max_input']}\")\n",
    "print(f\"Actual best value in training data (scaled): {results['actual_max_value']}\")\n",
    "print(f\"Actual best value in training data (original scale): {scaler.inverse_transform([[results['actual_max_value']]])[0][0]}\")\n",
    "print('\\nFunction Number:',function_num)\n",
    "format_results('Next query:', results['max_ei_input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "994de6f8-bcd0-43ab-98ac-e6ee5c43d2bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of initial X: (15, 3)\n",
      "Shape of initial y: (15,)\n",
      "After week 1:\n",
      "Shape of X: (16, 3)\n",
      "Shape of y: (16,)\n",
      "After week 2:\n",
      "Shape of X: (17, 3)\n",
      "Shape of y: (17,)\n",
      "After week 3:\n",
      "Shape of X: (18, 3)\n",
      "Shape of y: (18,)\n",
      "After week 4:\n",
      "Shape of X: (19, 3)\n",
      "Shape of y: (19,)\n",
      "After week 5:\n",
      "Shape of X: (20, 3)\n",
      "Shape of y: (20,)\n",
      "After week 6:\n",
      "Shape of X: (21, 3)\n",
      "Shape of y: (21,)\n",
      "After week 7:\n",
      "Shape of X: (22, 3)\n",
      "Shape of y: (22,)\n",
      "After week 8:\n",
      "Shape of X: (23, 3)\n",
      "Shape of y: (23,)\n",
      "\n",
      "added new data Batch week20\n",
      "Shape of initial X2: (15, 3)\n",
      "Shape of initial y2: (15,)\n",
      "\n",
      "Final DataFrame shape: (38, 4)\n",
      "          X1        X2        X3         y\n",
      "0   0.171525  0.343917  0.248737 -0.112122\n",
      "1   0.242114  0.644074  0.272433 -0.087963\n",
      "2   0.534906  0.398501  0.173389 -0.111415\n",
      "3   0.492581  0.611593  0.340176 -0.034835\n",
      "4   0.134622  0.219917  0.458206 -0.048008\n",
      "5   0.345523  0.941360  0.269363 -0.110621\n",
      "6   0.151837  0.439991  0.990882 -0.398926\n",
      "7   0.645503  0.397143  0.919771 -0.113869\n",
      "8   0.746912  0.284196  0.226300 -0.131461\n",
      "9   0.170477  0.697032  0.149169 -0.094190\n",
      "10  0.220549  0.297825  0.343555 -0.046947\n",
      "11  0.666014  0.671985  0.246295 -0.105965\n",
      "12  0.046809  0.231360  0.770618 -0.118048\n",
      "13  0.600097  0.725136  0.066089 -0.036378\n",
      "14  0.965995  0.861120  0.566829 -0.056758\n",
      "15  0.469388  0.102041  0.530612 -0.063759\n",
      "16  0.571429  0.755102  0.000000 -0.111727\n",
      "17  0.612245  0.714286  0.102041 -0.059114\n",
      "18  0.714286  0.612245  0.000000 -0.111977\n",
      "19  0.326531  0.306122  0.408163 -0.031368\n",
      "20  0.489796  0.999990  0.999999 -0.479587\n",
      "21  0.755102  0.612245  0.448980 -0.001021\n",
      "22  0.855102  0.712245  0.438776 -0.021626\n",
      "23  0.171525  0.343917  0.248737 -0.112122\n",
      "24  0.242114  0.644074  0.272433 -0.087963\n",
      "25  0.534906  0.398501  0.173389 -0.111415\n",
      "26  0.492581  0.611593  0.340176 -0.034835\n",
      "27  0.134622  0.219917  0.458206 -0.048008\n",
      "28  0.345523  0.941360  0.269363 -0.110621\n",
      "29  0.151837  0.439991  0.990882 -0.398926\n",
      "30  0.645503  0.397143  0.919771 -0.113869\n",
      "31  0.746912  0.284196  0.226300 -0.131461\n",
      "32  0.170477  0.697032  0.149169 -0.094190\n",
      "33  0.220549  0.297825  0.343555 -0.046947\n",
      "34  0.666014  0.671985  0.246295 -0.105965\n",
      "35  0.046809  0.231360  0.770618 -0.118048\n",
      "36  0.600097  0.725136  0.066089 -0.036378\n",
      "37  0.965995  0.861120  0.566829 -0.056758\n",
      "\n",
      "The maximum y is: -0.0010214738225351222\n",
      "\n",
      "The row with the max y is:\n",
      "          X1        X2       X3         y\n",
      "21  0.755102  0.612245  0.44898 -0.001021\n"
     ]
    }
   ],
   "source": [
    "function_num = 3\n",
    "num_week = 8\n",
    "\n",
    "X, y, df, max_y, max_row = load_and_process_data(function_num, num_week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "74a060dd-2ba2-43e9-80f2-c72649a3547a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 20/200 - Loss: 0.1235   Noise: 0.1371\n",
      "Iter 40/200 - Loss: -0.4622   Noise: 0.0237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\claud\\AppData\\Local\\Temp\\ipykernel_115208\\2251220785.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.X = torch.tensor(X, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 60/200 - Loss: -1.0302   Noise: 0.0045\n",
      "Iter 80/200 - Loss: -1.4964   Noise: 0.0008\n",
      "Iter 100/200 - Loss: -1.6588   Noise: 0.0003\n",
      "Iter 120/200 - Loss: -1.6759   Noise: 0.0002\n",
      "Iter 140/200 - Loss: -1.6841   Noise: 0.0002\n",
      "Iter 160/200 - Loss: -1.7704   Noise: 0.0002\n",
      "Iter 180/200 - Loss: -1.8130   Noise: 0.0001\n",
      "Iter 200/200 - Loss: -1.8357   Noise: 0.0001\n",
      "Optimization Results:\n",
      "Best point found: [0.85510198 0.59387768 0.50408204]\n",
      "Predicted value at best point (scaled): 0.9844790697097778\n",
      "Predicted value at best point (original scale): -0.008449256291857136\n",
      "Expected Improvement: 0.002981931436806917\n",
      "\n",
      "Actual best point in training data: [0.755102 0.612245 0.44898 ]\n",
      "Actual best value in training data (scaled): 1.0\n",
      "Actual best value in training data (original scale): -0.001021473822535214\n",
      "\n",
      "Function Number: 3\n",
      "Next query:\n",
      "[0.855102-0.593878-0.504082]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Scale y values\n",
    "scaler = MinMaxScaler()\n",
    "y_scaled = scaler.fit_transform(y.reshape(-1, 1)).flatten()\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y_scaled = torch.tensor(y_scaled, dtype=torch.float32)\n",
    "\n",
    "# Create and use the optimizer\n",
    "optimizer = BayesianOptimizer(X, y_scaled)\n",
    "optimizer.train(training_iter=200)\n",
    "results = optimizer.optimize(num_points=50, hypercube_size=0.2)\n",
    "\n",
    "print(\"Optimization Results:\")\n",
    "print(f\"Best point found: {results['max_ei_input']}\")\n",
    "print(f\"Predicted value at best point (scaled): {results['max_ei_prediction']}\")\n",
    "print(f\"Predicted value at best point (original scale): {scaler.inverse_transform([[results['max_ei_prediction']]])[0][0]}\")\n",
    "print(f\"Expected Improvement: {results['max_ei_value']}\")\n",
    "print(f\"\\nActual best point in training data: {results['actual_max_input']}\")\n",
    "print(f\"Actual best value in training data (scaled): {results['actual_max_value']}\")\n",
    "print(f\"Actual best value in training data (original scale): {scaler.inverse_transform([[results['actual_max_value']]])[0][0]}\")\n",
    "print('\\nFunction Number:',function_num)\n",
    "format_results('Next query:', results['max_ei_input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "54c0e6a5-f00a-4321-a1fc-2c8db85c3dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of initial X: (30, 4)\n",
      "Shape of initial y: (30,)\n",
      "After week 1:\n",
      "Shape of X: (31, 4)\n",
      "Shape of y: (31,)\n",
      "After week 2:\n",
      "Shape of X: (32, 4)\n",
      "Shape of y: (32,)\n",
      "After week 3:\n",
      "Shape of X: (33, 4)\n",
      "Shape of y: (33,)\n",
      "After week 4:\n",
      "Shape of X: (34, 4)\n",
      "Shape of y: (34,)\n",
      "After week 5:\n",
      "Shape of X: (35, 4)\n",
      "Shape of y: (35,)\n",
      "After week 6:\n",
      "Shape of X: (36, 4)\n",
      "Shape of y: (36,)\n",
      "After week 7:\n",
      "Shape of X: (37, 4)\n",
      "Shape of y: (37,)\n",
      "After week 8:\n",
      "Shape of X: (38, 4)\n",
      "Shape of y: (38,)\n",
      "\n",
      "added new data Batch week20\n",
      "Shape of initial X2: (30, 4)\n",
      "Shape of initial y2: (30,)\n",
      "\n",
      "Final DataFrame shape: (68, 5)\n",
      "          X1        X2        X3        X4          y\n",
      "0   0.896981  0.725628  0.175404  0.701694 -22.108288\n",
      "1   0.889356  0.499588  0.539269  0.508783 -14.601397\n",
      "2   0.250946  0.033693  0.145380  0.494932 -11.699932\n",
      "3   0.346962  0.006250  0.760564  0.613024 -16.053765\n",
      "4   0.124871  0.129770  0.384400  0.287076 -10.069633\n",
      "..       ...       ...       ...       ...        ...\n",
      "63  0.948389  0.894513  0.851638  0.552196 -32.625660\n",
      "64  0.664955  0.046566  0.116777  0.793718 -19.989498\n",
      "65  0.577766  0.428772  0.425826  0.249007  -4.025542\n",
      "66  0.738613  0.482103  0.709366  0.503970 -13.122782\n",
      "67  0.854811  0.493965  0.735310  0.808092 -23.139428\n",
      "\n",
      "[68 rows x 5 columns]\n",
      "\n",
      "The maximum y is: 0.5591489570434409\n",
      "\n",
      "The row with the max y is:\n",
      "          X1        X2        X3        X4         y\n",
      "34  0.418571  0.397755  0.377347  0.418163  0.559149\n"
     ]
    }
   ],
   "source": [
    "function_num = 4\n",
    "num_week = 8\n",
    "\n",
    "X, y, df, max_y, max_row = load_and_process_data(function_num, num_week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "16d33b93-2883-4d50-853f-b509aead579b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 20/400 - Loss: 0.1331   Noise: 0.1372\n",
      "Iter 40/400 - Loss: -0.6301   Noise: 0.0198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\claud\\AppData\\Local\\Temp\\ipykernel_115208\\2251220785.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.X = torch.tensor(X, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 60/400 - Loss: -1.3405   Noise: 0.0029\n",
      "Iter 80/400 - Loss: -1.8369   Noise: 0.0006\n",
      "Iter 100/400 - Loss: -2.0850   Noise: 0.0002\n",
      "Iter 120/400 - Loss: -2.1696   Noise: 0.0001\n",
      "Iter 140/400 - Loss: -2.1991   Noise: 0.0001\n",
      "Iter 160/400 - Loss: -2.2126   Noise: 0.0001\n",
      "Iter 180/400 - Loss: -2.2202   Noise: 0.0001\n",
      "Iter 200/400 - Loss: -2.2251   Noise: 0.0001\n",
      "Iter 220/400 - Loss: -2.2285   Noise: 0.0001\n",
      "Iter 240/400 - Loss: -2.2311   Noise: 0.0001\n",
      "Iter 260/400 - Loss: -2.2330   Noise: 0.0001\n",
      "Iter 280/400 - Loss: -2.2345   Noise: 0.0001\n",
      "Iter 300/400 - Loss: -2.2358   Noise: 0.0001\n",
      "Iter 320/400 - Loss: -2.2368   Noise: 0.0001\n",
      "Iter 340/400 - Loss: -2.2376   Noise: 0.0001\n",
      "Iter 360/400 - Loss: -2.2383   Noise: 0.0001\n",
      "Iter 380/400 - Loss: -2.2389   Noise: 0.0001\n",
      "Iter 400/400 - Loss: -2.2395   Noise: 0.0001\n",
      "Optimization Results:\n",
      "Best point found: [0.42163222 0.40081622 0.33142863 0.42734668]\n",
      "Predicted value at best point (scaled): 0.987604022026062\n",
      "Predicted value at best point (original scale): 0.14779079346552554\n",
      "Expected Improvement: 8.525390876457095e-05\n",
      "\n",
      "Actual best point in training data: [0.418571 0.397755 0.377347 0.418163]\n",
      "Actual best value in training data (scaled): 1.0\n",
      "Actual best value in training data (original scale): 0.5591489570434419\n",
      "\n",
      "Function Number: 4\n",
      "Next query:\n",
      "[0.421632-0.400816-0.331429-0.427347]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Scale y values\n",
    "scaler = MinMaxScaler()\n",
    "y_scaled = scaler.fit_transform(y.reshape(-1, 1)).flatten()\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y_scaled = torch.tensor(y_scaled, dtype=torch.float32)\n",
    "\n",
    "# Create and use the optimizer\n",
    "optimizer = BayesianOptimizer(X, y_scaled)\n",
    "optimizer.train(training_iter=400)\n",
    "results = optimizer.optimize(num_points=50, hypercube_size=0.3)\n",
    "\n",
    "print(\"Optimization Results:\")\n",
    "print(f\"Best point found: {results['max_ei_input']}\")\n",
    "print(f\"Predicted value at best point (scaled): {results['max_ei_prediction']}\")\n",
    "print(f\"Predicted value at best point (original scale): {scaler.inverse_transform([[results['max_ei_prediction']]])[0][0]}\")\n",
    "print(f\"Expected Improvement: {results['max_ei_value']}\")\n",
    "print(f\"\\nActual best point in training data: {results['actual_max_input']}\")\n",
    "print(f\"Actual best value in training data (scaled): {results['actual_max_value']}\")\n",
    "print(f\"Actual best value in training data (original scale): {scaler.inverse_transform([[results['actual_max_value']]])[0][0]}\")\n",
    "print('\\nFunction Number:',function_num)\n",
    "format_results('Next query:', results['max_ei_input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6b57487f-96cd-4e82-b39b-02cb3ff298a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of initial X: (20, 4)\n",
      "Shape of initial y: (20,)\n",
      "After week 1:\n",
      "Shape of X: (21, 4)\n",
      "Shape of y: (21,)\n",
      "After week 2:\n",
      "Shape of X: (22, 4)\n",
      "Shape of y: (22,)\n",
      "After week 3:\n",
      "Shape of X: (23, 4)\n",
      "Shape of y: (23,)\n",
      "After week 4:\n",
      "Shape of X: (24, 4)\n",
      "Shape of y: (24,)\n",
      "After week 5:\n",
      "Shape of X: (25, 4)\n",
      "Shape of y: (25,)\n",
      "After week 6:\n",
      "Shape of X: (26, 4)\n",
      "Shape of y: (26,)\n",
      "After week 7:\n",
      "Shape of X: (27, 4)\n",
      "Shape of y: (27,)\n",
      "After week 8:\n",
      "Shape of X: (28, 4)\n",
      "Shape of y: (28,)\n",
      "\n",
      "added new data Batch week20\n",
      "Shape of initial X2: (20, 4)\n",
      "Shape of initial y2: (20,)\n",
      "\n",
      "Final DataFrame shape: (48, 5)\n",
      "          X1        X2        X3        X4            y\n",
      "0   0.191447  0.038193  0.607418  0.414584    64.443440\n",
      "1   0.758653  0.536518  0.656000  0.360342    18.301380\n",
      "2   0.438350  0.804340  0.210245  0.151295     0.112940\n",
      "3   0.706051  0.534192  0.264243  0.482088     4.210898\n",
      "4   0.836478  0.193610  0.663893  0.785649   258.370525\n",
      "5   0.683432  0.118663  0.829046  0.567577    78.434389\n",
      "6   0.553621  0.667350  0.323806  0.814870    57.571537\n",
      "7   0.352356  0.322242  0.116979  0.473113   109.571876\n",
      "8   0.153786  0.729382  0.422598  0.443074     8.847992\n",
      "9   0.463442  0.630025  0.107906  0.957644   233.223610\n",
      "10  0.677491  0.358510  0.479592  0.072880    24.423088\n",
      "11  0.583973  0.147243  0.348097  0.428615    64.420147\n",
      "12  0.306889  0.316878  0.622634  0.095399    63.476716\n",
      "13  0.511142  0.817957  0.728710  0.112354    79.729130\n",
      "14  0.438933  0.774092  0.378167  0.933696   355.806818\n",
      "15  0.224189  0.846480  0.879484  0.878516  1088.859618\n",
      "16  0.725262  0.479870  0.088947  0.759760    28.866752\n",
      "17  0.355482  0.639619  0.417618  0.122604    45.181570\n",
      "18  0.119879  0.862540  0.643331  0.849804   431.612757\n",
      "19  0.126885  0.153430  0.770162  0.190518     9.972332\n",
      "20  0.224490  0.836735  0.877551  0.877551  1035.787416\n",
      "21  0.224490  0.836735  0.877551  0.877551  1035.787416\n",
      "22  0.224490  0.857143  0.897959  0.877551  1223.352760\n",
      "23  0.224490  0.857143  0.918367  0.877551  1337.576725\n",
      "24  0.224490  0.857143  0.938776  0.897959  1581.754867\n",
      "25  0.224490  0.857143  0.959184  0.918367  1865.088705\n",
      "26  0.224490  0.857143  0.959184  0.918367  1865.088705\n",
      "27  0.199490  0.882143  0.984184  0.943367  2437.767773\n",
      "28  0.191447  0.038193  0.607418  0.414584    64.443440\n",
      "29  0.758653  0.536518  0.656000  0.360342    18.301380\n",
      "30  0.438350  0.804340  0.210245  0.151295     0.112940\n",
      "31  0.706051  0.534192  0.264243  0.482088     4.210898\n",
      "32  0.836478  0.193610  0.663893  0.785649   258.370525\n",
      "33  0.683432  0.118663  0.829046  0.567577    78.434389\n",
      "34  0.553621  0.667350  0.323806  0.814870    57.571537\n",
      "35  0.352356  0.322242  0.116979  0.473113   109.571876\n",
      "36  0.153786  0.729382  0.422598  0.443074     8.847992\n",
      "37  0.463442  0.630025  0.107906  0.957644   233.223610\n",
      "38  0.677491  0.358510  0.479592  0.072880    24.423088\n",
      "39  0.583973  0.147243  0.348097  0.428615    64.420147\n",
      "40  0.306889  0.316878  0.622634  0.095399    63.476716\n",
      "41  0.511142  0.817957  0.728710  0.112354    79.729130\n",
      "42  0.438933  0.774092  0.378167  0.933696   355.806818\n",
      "43  0.224189  0.846480  0.879484  0.878516  1088.859618\n",
      "44  0.725262  0.479870  0.088947  0.759760    28.866752\n",
      "45  0.355482  0.639619  0.417618  0.122604    45.181570\n",
      "46  0.119879  0.862540  0.643331  0.849804   431.612757\n",
      "47  0.126885  0.153430  0.770162  0.190518     9.972332\n",
      "\n",
      "The maximum y is: 2437.7677729094985\n",
      "\n",
      "The row with the max y is:\n",
      "         X1        X2        X3        X4            y\n",
      "27  0.19949  0.882143  0.984184  0.943367  2437.767773\n"
     ]
    }
   ],
   "source": [
    "function_num = 5\n",
    "num_week = 8\n",
    "\n",
    "X, y, df, max_y, max_row = load_and_process_data(function_num, num_week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "16f47cd0-30d1-4314-a07e-ea8dc524b24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 20/500 - Loss: 0.0960   Noise: 0.1360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\claud\\AppData\\Local\\Temp\\ipykernel_115208\\2251220785.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.X = torch.tensor(X, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 40/500 - Loss: -0.6411   Noise: 0.0197\n",
      "Iter 60/500 - Loss: -1.2156   Noise: 0.0033\n",
      "Iter 80/500 - Loss: -1.5372   Noise: 0.0008\n",
      "Iter 100/500 - Loss: -1.6785   Noise: 0.0004\n",
      "Iter 120/500 - Loss: -1.7736   Noise: 0.0002\n",
      "Iter 140/500 - Loss: -1.8118   Noise: 0.0002\n",
      "Iter 160/500 - Loss: -1.8295   Noise: 0.0002\n",
      "Iter 180/500 - Loss: -1.8398   Noise: 0.0001\n",
      "Iter 200/500 - Loss: -1.8466   Noise: 0.0001\n",
      "Iter 220/500 - Loss: -1.8514   Noise: 0.0001\n",
      "Iter 240/500 - Loss: -1.8553   Noise: 0.0001\n",
      "Iter 260/500 - Loss: -1.8589   Noise: 0.0001\n",
      "Iter 280/500 - Loss: -1.8618   Noise: 0.0001\n",
      "Iter 300/500 - Loss: -1.8639   Noise: 0.0001\n",
      "Iter 320/500 - Loss: -1.8657   Noise: 0.0001\n",
      "Iter 340/500 - Loss: -1.8674   Noise: 0.0001\n",
      "Iter 360/500 - Loss: -1.8687   Noise: 0.0001\n",
      "Iter 380/500 - Loss: -1.8707   Noise: 0.0001\n",
      "Iter 400/500 - Loss: -1.8721   Noise: 0.0001\n",
      "Iter 420/500 - Loss: -1.8732   Noise: 0.0001\n",
      "Iter 440/500 - Loss: -1.8741   Noise: 0.0001\n",
      "Iter 460/500 - Loss: -1.8749   Noise: 0.0001\n",
      "Iter 480/500 - Loss: -1.8757   Noise: 0.0001\n",
      "Iter 500/500 - Loss: -1.8769   Noise: 0.0001\n",
      "Optimization Results:\n",
      "Best point found: [0.1770541  0.90457892 1.         0.968367  ]\n",
      "Predicted value at best point (scaled): 1.1082583665847778\n",
      "Predicted value at best point (original scale): 2701.664303439923\n",
      "Expected Improvement: 0.0982583686709404\n",
      "\n",
      "Actual best point in training data: [0.19949  0.882143 0.984184 0.943367]\n",
      "Actual best value in training data (scaled): 1.0\n",
      "Actual best value in training data (original scale): 2437.767772909498\n",
      "\n",
      "Function Number: 5\n",
      "Next query:\n",
      "[0.177054-0.904579-1.000000-0.968367]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Scale y values\n",
    "scaler = MinMaxScaler()\n",
    "y_scaled = scaler.fit_transform(y.reshape(-1, 1)).flatten()\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y_scaled = torch.tensor(y_scaled, dtype=torch.float32)\n",
    "\n",
    "# Create and use the optimizer\n",
    "optimizer = BayesianOptimizer(X, y_scaled)\n",
    "optimizer.train(training_iter=500)\n",
    "results = optimizer.optimize(num_points=40, hypercube_size=0.05)\n",
    "\n",
    "print(\"Optimization Results:\")\n",
    "print(f\"Best point found: {results['max_ei_input']}\")\n",
    "print(f\"Predicted value at best point (scaled): {results['max_ei_prediction']}\")\n",
    "print(f\"Predicted value at best point (original scale): {scaler.inverse_transform([[results['max_ei_prediction']]])[0][0]}\")\n",
    "print(f\"Expected Improvement: {results['max_ei_value']}\")\n",
    "print(f\"\\nActual best point in training data: {results['actual_max_input']}\")\n",
    "print(f\"Actual best value in training data (scaled): {results['actual_max_value']}\")\n",
    "print(f\"Actual best value in training data (original scale): {scaler.inverse_transform([[results['actual_max_value']]])[0][0]}\")\n",
    "print('\\nFunction Number:',function_num)\n",
    "format_results('Next query:', results['max_ei_input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2c10e080-7168-4b8a-9f44-de6bba8c8aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of initial X: (20, 5)\n",
      "Shape of initial y: (20,)\n",
      "After week 1:\n",
      "Shape of X: (21, 5)\n",
      "Shape of y: (21,)\n",
      "After week 2:\n",
      "Shape of X: (22, 5)\n",
      "Shape of y: (22,)\n",
      "After week 3:\n",
      "Shape of X: (23, 5)\n",
      "Shape of y: (23,)\n",
      "After week 4:\n",
      "Shape of X: (24, 5)\n",
      "Shape of y: (24,)\n",
      "After week 5:\n",
      "Shape of X: (25, 5)\n",
      "Shape of y: (25,)\n",
      "After week 6:\n",
      "Shape of X: (26, 5)\n",
      "Shape of y: (26,)\n",
      "After week 7:\n",
      "Shape of X: (27, 5)\n",
      "Shape of y: (27,)\n",
      "After week 8:\n",
      "Shape of X: (28, 5)\n",
      "Shape of y: (28,)\n",
      "\n",
      "added new data Batch week20\n",
      "Shape of initial X2: (20, 5)\n",
      "Shape of initial y2: (20,)\n",
      "\n",
      "Final DataFrame shape: (48, 6)\n",
      "          X1        X2        X3        X4        X5         y\n",
      "0   0.728186  0.154693  0.732552  0.693997  0.056401 -0.714265\n",
      "1   0.242384  0.844100  0.577809  0.679021  0.501953 -1.209955\n",
      "2   0.729523  0.748106  0.679775  0.356552  0.671054 -1.672200\n",
      "3   0.770620  0.114404  0.046780  0.648324  0.273549 -1.536058\n",
      "4   0.618812  0.331802  0.187288  0.756238  0.328835 -0.829237\n",
      "5   0.784958  0.910682  0.708120  0.959225  0.004911 -1.247049\n",
      "6   0.145111  0.896685  0.896322  0.726272  0.236272 -1.233786\n",
      "7   0.945069  0.288459  0.978806  0.961656  0.598016 -1.694343\n",
      "8   0.125720  0.862725  0.028544  0.246605  0.751206 -2.571170\n",
      "9   0.757594  0.355831  0.016523  0.434207  0.112433 -1.309116\n",
      "10  0.536797  0.308781  0.411879  0.388225  0.522528 -1.144785\n",
      "11  0.957740  0.235669  0.099146  0.156806  0.071317 -1.912677\n",
      "12  0.629308  0.803484  0.811408  0.045613  0.110624 -1.622839\n",
      "13  0.021735  0.428084  0.835939  0.489489  0.511082 -1.356682\n",
      "14  0.439344  0.698924  0.426820  0.109476  0.877888 -2.018425\n",
      "15  0.258906  0.793678  0.642114  0.196673  0.593103 -1.702558\n",
      "16  0.432166  0.715618  0.341819  0.705000  0.614962 -1.294247\n",
      "17  0.782880  0.536336  0.443284  0.859700  0.010326 -0.935757\n",
      "18  0.921776  0.931871  0.414876  0.595057  0.735626 -2.155768\n",
      "19  0.126679  0.291470  0.064528  0.680515  0.892819 -1.746882\n",
      "20  0.473684  0.421053  0.315789  0.842105  0.263158 -0.754840\n",
      "21  0.684211  0.052632  0.842105  0.631579  0.000000 -1.084899\n",
      "22  0.628186  0.054693  0.632552  0.593997  0.156401 -0.808452\n",
      "23  0.678186  0.204693  0.782552  0.743997  0.006401 -0.603483\n",
      "24  0.678186  0.254693  0.732552  0.793997  0.026600 -0.500936\n",
      "25  0.631579  0.263158  0.684211  0.842105  0.052632 -0.497229\n",
      "26  0.581579  0.363158  0.684211  0.742105  0.114474 -0.220968\n",
      "27  0.536751  0.401089  0.611797  0.649002  0.090336 -0.391611\n",
      "28  0.728186  0.154693  0.732552  0.693997  0.056401 -0.714265\n",
      "29  0.242384  0.844100  0.577809  0.679021  0.501953 -1.209955\n",
      "30  0.729523  0.748106  0.679775  0.356552  0.671054 -1.672200\n",
      "31  0.770620  0.114404  0.046780  0.648324  0.273549 -1.536058\n",
      "32  0.618812  0.331802  0.187288  0.756238  0.328835 -0.829237\n",
      "33  0.784958  0.910682  0.708120  0.959225  0.004911 -1.247049\n",
      "34  0.145111  0.896685  0.896322  0.726272  0.236272 -1.233786\n",
      "35  0.945069  0.288459  0.978806  0.961656  0.598016 -1.694343\n",
      "36  0.125720  0.862725  0.028544  0.246605  0.751206 -2.571170\n",
      "37  0.757594  0.355831  0.016523  0.434207  0.112433 -1.309116\n",
      "38  0.536797  0.308781  0.411879  0.388225  0.522528 -1.144785\n",
      "39  0.957740  0.235669  0.099146  0.156806  0.071317 -1.912677\n",
      "40  0.629308  0.803484  0.811408  0.045613  0.110624 -1.622839\n",
      "41  0.021735  0.428084  0.835939  0.489489  0.511082 -1.356682\n",
      "42  0.439344  0.698924  0.426820  0.109476  0.877888 -2.018425\n",
      "43  0.258906  0.793678  0.642114  0.196673  0.593103 -1.702558\n",
      "44  0.432166  0.715618  0.341819  0.705000  0.614962 -1.294247\n",
      "45  0.782880  0.536336  0.443284  0.859700  0.010326 -0.935757\n",
      "46  0.921776  0.931871  0.414876  0.595057  0.735626 -2.155768\n",
      "47  0.126679  0.291470  0.064528  0.680515  0.892819 -1.746882\n",
      "\n",
      "The maximum y is: -0.22096796504637056\n",
      "\n",
      "The row with the max y is:\n",
      "          X1        X2        X3        X4        X5         y\n",
      "26  0.581579  0.363158  0.684211  0.742105  0.114474 -0.220968\n"
     ]
    }
   ],
   "source": [
    "function_num = 6\n",
    "num_week = 8\n",
    "\n",
    "X, y, df, max_y, max_row = load_and_process_data(function_num, num_week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7bdf639e-1992-4a90-ad06-43fd97a6c927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 20/500 - Loss: 0.1052   Noise: 0.1356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\claud\\AppData\\Local\\Temp\\ipykernel_115208\\2251220785.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.X = torch.tensor(X, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 40/500 - Loss: -0.4870   Noise: 0.0211\n",
      "Iter 60/500 - Loss: -1.0064   Noise: 0.0047\n",
      "Iter 80/500 - Loss: -1.2881   Noise: 0.0012\n",
      "Iter 100/500 - Loss: -1.5338   Noise: 0.0005\n",
      "Iter 120/500 - Loss: -1.7088   Noise: 0.0002\n",
      "Iter 140/500 - Loss: -1.7857   Noise: 0.0001\n",
      "Iter 160/500 - Loss: -1.8144   Noise: 0.0001\n",
      "Iter 180/500 - Loss: -1.8271   Noise: 0.0001\n",
      "Iter 200/500 - Loss: -1.8341   Noise: 0.0001\n",
      "Iter 220/500 - Loss: -1.8385   Noise: 0.0001\n",
      "Iter 240/500 - Loss: -1.8415   Noise: 0.0001\n",
      "Iter 260/500 - Loss: -1.8437   Noise: 0.0001\n",
      "Iter 280/500 - Loss: -1.8453   Noise: 0.0001\n",
      "Iter 300/500 - Loss: -1.8466   Noise: 0.0001\n",
      "Iter 320/500 - Loss: -1.8477   Noise: 0.0001\n",
      "Iter 340/500 - Loss: -1.8485   Noise: 0.0001\n",
      "Iter 360/500 - Loss: -1.8492   Noise: 0.0001\n",
      "Iter 380/500 - Loss: -1.8498   Noise: 0.0001\n",
      "Iter 400/500 - Loss: -1.8503   Noise: 0.0001\n",
      "Iter 420/500 - Loss: -1.8508   Noise: 0.0001\n",
      "Iter 440/500 - Loss: -1.8511   Noise: 0.0001\n",
      "Iter 460/500 - Loss: -1.8515   Noise: 0.0001\n",
      "Iter 480/500 - Loss: -1.8518   Noise: 0.0001\n",
      "Iter 500/500 - Loss: -1.8518   Noise: 0.0001\n",
      "Optimization Results:\n",
      "Best point found: [0.59882041 0.40108902 0.78421102 0.72486363 0.214474  ]\n",
      "Predicted value at best point (scaled): 1.047950029373169\n",
      "Predicted value at best point (original scale): -0.108275726101864\n",
      "Expected Improvement: 0.04084905982017517\n",
      "\n",
      "Actual best point in training data: [0.581579 0.363158 0.684211 0.742105 0.114474]\n",
      "Actual best value in training data (scaled): 1.0\n",
      "Actual best value in training data (original scale): -0.22096796504637065\n",
      "\n",
      "Function Number: 6\n",
      "Next query:\n",
      "[0.598820-0.401089-0.784211-0.724864-0.214474]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Scale y values\n",
    "scaler = MinMaxScaler()\n",
    "y_scaled = scaler.fit_transform(y.reshape(-1, 1)).flatten()\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y_scaled = torch.tensor(y_scaled, dtype=torch.float32)\n",
    "\n",
    "# Create and use the optimizer\n",
    "optimizer = BayesianOptimizer(X, y_scaled)\n",
    "optimizer.train(training_iter=500)\n",
    "results = optimizer.optimize(num_points=30, hypercube_size=0.2)\n",
    "\n",
    "print(\"Optimization Results:\")\n",
    "print(f\"Best point found: {results['max_ei_input']}\")\n",
    "print(f\"Predicted value at best point (scaled): {results['max_ei_prediction']}\")\n",
    "print(f\"Predicted value at best point (original scale): {scaler.inverse_transform([[results['max_ei_prediction']]])[0][0]}\")\n",
    "print(f\"Expected Improvement: {results['max_ei_value']}\")\n",
    "print(f\"\\nActual best point in training data: {results['actual_max_input']}\")\n",
    "print(f\"Actual best value in training data (scaled): {results['actual_max_value']}\")\n",
    "print(f\"Actual best value in training data (original scale): {scaler.inverse_transform([[results['actual_max_value']]])[0][0]}\")\n",
    "print('\\nFunction Number:',function_num)\n",
    "format_results('Next query:', results['max_ei_input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0776cb2f-c6d8-4b90-8892-4825879a82d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of initial X: (30, 6)\n",
      "Shape of initial y: (30,)\n",
      "After week 1:\n",
      "Shape of X: (31, 6)\n",
      "Shape of y: (31,)\n",
      "After week 2:\n",
      "Shape of X: (32, 6)\n",
      "Shape of y: (32,)\n",
      "After week 3:\n",
      "Shape of X: (33, 6)\n",
      "Shape of y: (33,)\n",
      "After week 4:\n",
      "Shape of X: (34, 6)\n",
      "Shape of y: (34,)\n",
      "After week 5:\n",
      "Shape of X: (35, 6)\n",
      "Shape of y: (35,)\n",
      "After week 6:\n",
      "Shape of X: (36, 6)\n",
      "Shape of y: (36,)\n",
      "After week 7:\n",
      "Shape of X: (37, 6)\n",
      "Shape of y: (37,)\n",
      "After week 8:\n",
      "Shape of X: (38, 6)\n",
      "Shape of y: (38,)\n",
      "\n",
      "added new data Batch week20\n",
      "Shape of initial X2: (30, 6)\n",
      "Shape of initial y2: (30,)\n",
      "\n",
      "Final DataFrame shape: (68, 7)\n",
      "          X1        X2        X3        X4        X5        X6         y\n",
      "0   0.272624  0.324495  0.897109  0.832951  0.154063  0.795864  0.604433\n",
      "1   0.543003  0.924694  0.341567  0.646486  0.718440  0.343133  0.562753\n",
      "2   0.090832  0.661529  0.065931  0.258577  0.963453  0.640265  0.007503\n",
      "3   0.118867  0.615055  0.905816  0.855300  0.413631  0.585236  0.061424\n",
      "4   0.630218  0.838097  0.680013  0.731895  0.526737  0.348429  0.273047\n",
      "..       ...       ...       ...       ...       ...       ...       ...\n",
      "63  0.066611  0.528045  0.816095  0.961017  0.086509  0.777788  0.516457\n",
      "64  0.932466  0.488812  0.258608  0.956243  0.190428  0.519852  0.003777\n",
      "65  0.846867  0.142429  0.060669  0.756292  0.552398  0.081306  0.003134\n",
      "66  0.806282  0.324122  0.726076  0.148712  0.719376  0.362884  0.021343\n",
      "67  0.476823  0.340942  0.014335  0.880140  0.998655  0.079664  0.095411\n",
      "\n",
      "[68 rows x 7 columns]\n",
      "\n",
      "The maximum y is: 2.2970015668039387\n",
      "\n",
      "The row with the max y is:\n",
      "          X1        X2        X3       X4        X5        X6         y\n",
      "37  0.222222  0.233333  0.433333  0.08254  0.290476  0.652381  2.297002\n"
     ]
    }
   ],
   "source": [
    "function_num = 7\n",
    "num_week = 8\n",
    "\n",
    "X, y, df, max_y, max_row = load_and_process_data(function_num, num_week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dfe969b8-fe1e-45d3-9902-d529101ee189",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\claud\\AppData\\Local\\Temp\\ipykernel_115208\\2251220785.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.X = torch.tensor(X, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 20/500 - Loss: 0.1438   Noise: 0.1365\n",
      "Iter 40/500 - Loss: -0.3171   Noise: 0.0263\n",
      "Iter 60/500 - Loss: -0.9123   Noise: 0.0055\n",
      "Iter 80/500 - Loss: -1.3867   Noise: 0.0010\n",
      "Iter 100/500 - Loss: -1.6970   Noise: 0.0003\n",
      "Iter 120/500 - Loss: -1.8277   Noise: 0.0002\n",
      "Iter 140/500 - Loss: -1.8711   Noise: 0.0001\n",
      "Iter 160/500 - Loss: -1.8882   Noise: 0.0001\n",
      "Iter 180/500 - Loss: -1.8969   Noise: 0.0001\n",
      "Iter 200/500 - Loss: -1.9021   Noise: 0.0001\n",
      "Iter 220/500 - Loss: -1.9056   Noise: 0.0001\n",
      "Iter 240/500 - Loss: -1.9081   Noise: 0.0001\n",
      "Iter 260/500 - Loss: -1.9100   Noise: 0.0001\n",
      "Iter 280/500 - Loss: -1.9115   Noise: 0.0001\n",
      "Iter 300/500 - Loss: -1.9126   Noise: 0.0001\n",
      "Iter 320/500 - Loss: -1.9136   Noise: 0.0001\n",
      "Iter 340/500 - Loss: -1.9144   Noise: 0.0001\n",
      "Iter 360/500 - Loss: -1.9151   Noise: 0.0001\n",
      "Iter 380/500 - Loss: -1.9156   Noise: 0.0001\n",
      "Iter 400/500 - Loss: -1.9161   Noise: 0.0001\n",
      "Iter 420/500 - Loss: -1.9165   Noise: 0.0001\n",
      "Iter 440/500 - Loss: -1.9169   Noise: 0.0001\n",
      "Iter 460/500 - Loss: -1.9172   Noise: 0.0001\n",
      "Iter 480/500 - Loss: -1.9175   Noise: 0.0001\n",
      "Iter 500/500 - Loss: -1.9175   Noise: 0.0001\n",
      "Optimization Results:\n",
      "Best point found: [0.222222   0.13333301 0.53333301 0.01303857 0.26190457 0.652381  ]\n",
      "Predicted value at best point (scaled): 1.0009987354278564\n",
      "Predicted value at best point (original scale): 2.299292965597721\n",
      "Expected Improvement: 0.01086549460887909\n",
      "\n",
      "Actual best point in training data: [0.222222 0.233333 0.433333 0.08254  0.290476 0.652381]\n",
      "Actual best value in training data (scaled): 1.0\n",
      "Actual best value in training data (original scale): 2.2970015668039387\n",
      "\n",
      "Function Number: 7\n",
      "Next query:\n",
      "[0.222222-0.133333-0.533333-0.013039-0.261905-0.652381]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Scale y values\n",
    "scaler = MinMaxScaler()\n",
    "y_scaled = scaler.fit_transform(y.reshape(-1, 1)).flatten()\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y_scaled = torch.tensor(y_scaled, dtype=torch.float32)\n",
    "\n",
    "# Create and use the optimizer\n",
    "optimizer = BayesianOptimizer(X, y_scaled)\n",
    "optimizer.train(training_iter=500)\n",
    "results = optimizer.optimize(num_points=15, hypercube_size=0.2)\n",
    "\n",
    "print(\"Optimization Results:\")\n",
    "print(f\"Best point found: {results['max_ei_input']}\")\n",
    "print(f\"Predicted value at best point (scaled): {results['max_ei_prediction']}\")\n",
    "print(f\"Predicted value at best point (original scale): {scaler.inverse_transform([[results['max_ei_prediction']]])[0][0]}\")\n",
    "print(f\"Expected Improvement: {results['max_ei_value']}\")\n",
    "print(f\"\\nActual best point in training data: {results['actual_max_input']}\")\n",
    "print(f\"Actual best value in training data (scaled): {results['actual_max_value']}\")\n",
    "print(f\"Actual best value in training data (original scale): {scaler.inverse_transform([[results['actual_max_value']]])[0][0]}\")\n",
    "print('\\nFunction Number:',function_num)\n",
    "format_results('Next query:', results['max_ei_input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e16dd86e-796b-48f1-8775-1f40169835db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of initial X: (40, 8)\n",
      "Shape of initial y: (40,)\n",
      "After week 1:\n",
      "Shape of X: (41, 8)\n",
      "Shape of y: (41,)\n",
      "After week 2:\n",
      "Shape of X: (42, 8)\n",
      "Shape of y: (42,)\n",
      "After week 3:\n",
      "Shape of X: (43, 8)\n",
      "Shape of y: (43,)\n",
      "After week 4:\n",
      "Shape of X: (44, 8)\n",
      "Shape of y: (44,)\n",
      "After week 5:\n",
      "Shape of X: (45, 8)\n",
      "Shape of y: (45,)\n",
      "After week 6:\n",
      "Shape of X: (46, 8)\n",
      "Shape of y: (46,)\n",
      "After week 7:\n",
      "Shape of X: (47, 8)\n",
      "Shape of y: (47,)\n",
      "After week 8:\n",
      "Shape of X: (48, 8)\n",
      "Shape of y: (48,)\n",
      "\n",
      "added new data Batch week20\n",
      "Shape of initial X2: (40, 8)\n",
      "Shape of initial y2: (40,)\n",
      "\n",
      "Final DataFrame shape: (88, 9)\n",
      "          X1        X2        X3        X4        X5        X6        X7  \\\n",
      "0   0.604994  0.292215  0.908453  0.355506  0.201669  0.575338  0.310311   \n",
      "1   0.178007  0.566223  0.994862  0.210325  0.320153  0.707909  0.635384   \n",
      "2   0.009077  0.811626  0.520520  0.075687  0.265112  0.091652  0.592415   \n",
      "3   0.506028  0.653730  0.363411  0.177981  0.093728  0.197425  0.755827   \n",
      "4   0.359909  0.249076  0.495997  0.709215  0.114987  0.289207  0.557295   \n",
      "..       ...       ...       ...       ...       ...       ...       ...   \n",
      "83  0.472071  0.168203  0.086428  0.452656  0.480619  0.622439  0.928974   \n",
      "84  0.856007  0.638894  0.326192  0.668503  0.240298  0.210299  0.167546   \n",
      "85  0.810032  0.635046  0.269548  0.869605  0.661922  0.252259  0.765670   \n",
      "86  0.796253  0.007037  0.355697  0.487566  0.740520  0.706650  0.992914   \n",
      "87  0.481245  0.102461  0.219486  0.677322  0.247509  0.244341  0.163825   \n",
      "\n",
      "          X8         y  \n",
      "0   0.734281  7.398721  \n",
      "1   0.107132  7.005227  \n",
      "2   0.367320  8.459482  \n",
      "3   0.292472  8.284008  \n",
      "4   0.593882  8.606117  \n",
      "..       ...       ...  \n",
      "83  0.112536  8.472936  \n",
      "84  0.963590  7.977685  \n",
      "85  0.890549  7.460872  \n",
      "86  0.381734  7.436594  \n",
      "87  0.715962  9.183005  \n",
      "\n",
      "[88 rows x 9 columns]\n",
      "\n",
      "The maximum y is: 9.788479410642\n",
      "\n",
      "The row with the max y is:\n",
      "      X1    X2    X3   X4        X5        X6        X7        X8         y\n",
      "47  0.05  0.05  0.05  0.0  0.253935  0.551055  0.238307  0.643085  9.788479\n"
     ]
    }
   ],
   "source": [
    "function_num = 8\n",
    "num_week = 8\n",
    "\n",
    "X, y, df, max_y, max_row = load_and_process_data(function_num, num_week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "49b33182-fc21-49c9-8847-7af2a271fa65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\claud\\AppData\\Local\\Temp\\ipykernel_115208\\2251220785.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.X = torch.tensor(X, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 20/500 - Loss: 0.0601   Noise: 0.1343\n",
      "Iter 40/500 - Loss: -0.8084   Noise: 0.0176\n",
      "Iter 60/500 - Loss: -1.5154   Noise: 0.0026\n",
      "Iter 80/500 - Loss: -2.0467   Noise: 0.0005\n",
      "Iter 100/500 - Loss: -2.3532   Noise: 0.0002\n",
      "Iter 120/500 - Loss: -2.4727   Noise: 0.0001\n",
      "Iter 140/500 - Loss: -2.5170   Noise: 0.0001\n",
      "Iter 160/500 - Loss: -2.5375   Noise: 0.0001\n",
      "Iter 180/500 - Loss: -2.5493   Noise: 0.0001\n",
      "Iter 200/500 - Loss: -2.5569   Noise: 0.0001\n",
      "Iter 220/500 - Loss: -2.5624   Noise: 0.0001\n",
      "Iter 240/500 - Loss: -2.5665   Noise: 0.0001\n",
      "Iter 260/500 - Loss: -2.5697   Noise: 0.0001\n",
      "Iter 280/500 - Loss: -2.5722   Noise: 0.0001\n",
      "Iter 300/500 - Loss: -2.5744   Noise: 0.0001\n",
      "Iter 320/500 - Loss: -2.5762   Noise: 0.0001\n",
      "Iter 340/500 - Loss: -2.5778   Noise: 0.0001\n",
      "Iter 360/500 - Loss: -2.5790   Noise: 0.0001\n",
      "Iter 380/500 - Loss: -2.5801   Noise: 0.0001\n",
      "Iter 400/500 - Loss: -2.5812   Noise: 0.0001\n",
      "Iter 420/500 - Loss: -2.5822   Noise: 0.0001\n",
      "Iter 440/500 - Loss: -2.5831   Noise: 0.0001\n",
      "Iter 460/500 - Loss: -2.5838   Noise: 0.0001\n",
      "Iter 480/500 - Loss: -2.5845   Noise: 0.0001\n",
      "Iter 500/500 - Loss: -2.5851   Noise: 0.0001\n",
      "Optimization Results:\n",
      "Best point found: [0.1        0.075      0.1        0.         0.30393501 0.50105501\n",
      " 0.188307   0.693085  ]\n",
      "Predicted value at best point (scaled): 1.022883653640747\n",
      "Predicted value at best point (original scale): 9.8845057665264\n",
      "Expected Improvement: 0.014074169099330902\n",
      "\n",
      "Actual best point in training data: [0.05     0.05     0.05     0.       0.253935 0.551055 0.238307 0.643085]\n",
      "Actual best value in training data (scaled): 1.0\n",
      "Actual best value in training data (original scale): 9.788479410642\n",
      "\n",
      "Function Number: 8\n",
      "Next query:\n",
      "[0.100000-0.075000-0.100000-0.000000-0.303935-0.501055-0.188307-0.693085]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Scale y values\n",
    "scaler = MinMaxScaler()\n",
    "y_scaled = scaler.fit_transform(y.reshape(-1, 1)).flatten()\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y_scaled = torch.tensor(y_scaled, dtype=torch.float32)\n",
    "\n",
    "# Create and use the optimizer\n",
    "optimizer = BayesianOptimizer(X, y_scaled)\n",
    "optimizer.train(training_iter=500)\n",
    "results = optimizer.optimize(num_points=5, hypercube_size=0.1)\n",
    "\n",
    "print(\"Optimization Results:\")\n",
    "print(f\"Best point found: {results['max_ei_input']}\")\n",
    "print(f\"Predicted value at best point (scaled): {results['max_ei_prediction']}\")\n",
    "print(f\"Predicted value at best point (original scale): {scaler.inverse_transform([[results['max_ei_prediction']]])[0][0]}\")\n",
    "print(f\"Expected Improvement: {results['max_ei_value']}\")\n",
    "print(f\"\\nActual best point in training data: {results['actual_max_input']}\")\n",
    "print(f\"Actual best value in training data (scaled): {results['actual_max_value']}\")\n",
    "print(f\"Actual best value in training data (original scale): {scaler.inverse_transform([[results['actual_max_value']]])[0][0]}\")\n",
    "print('\\nFunction Number:',function_num)\n",
    "format_results('Next query:', results['max_ei_input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45902758-49b9-4a19-9583-166c724ff2c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
